{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use the trained KGQG model (T5 trained on WQ dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the model and tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5TokenizerFast\n",
    "model = T5ForConditionalGeneration.from_pretrained('stanlochten/t5-KGQgen')\n",
    "tokenizer = T5TokenizerFast.from_pretrained('t5-base',  extra_ids=0, \n",
    "            additional_special_tokens = ['<A>', '<H>', '<R>', '<T>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you load in your linearized graphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "# The model is trained on graphs that have the following form:\n",
    "# <A> answer node(s) <H> head <R> relation <T> tail <H> head <R> relation <T> tail ... etc ...\n",
    "\n",
    "graphs = ['<A> matt lanter <H> Star Wars: The Clone Wars <R> other crew <T> Matt Lanter <H> Star Wars: The Clone Wars <R> starring <T> Darth Vader',\n",
    "          '<A> parque warner madrid <H> Madrid <R> tourist attractions <T> Parque Warner Madrid <H> Parque Warner Madrid <R> rides <T> Batman: La Fuga',\n",
    "          '<A> ukhta moscow <H> Roman Abramovich <R> ships owned <T> Ecstasea <H> Roman Abramovich <R> places lived <T> Moscow <H> Roman Abramovich <R> places lived <T> Ukhta'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate a question for each graph, we first convert the text to token ids, then feed these ids to the model, and we decode the results to convert the token ids back to words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing...\n",
      "Predicting...\n",
      "Decoding...\n"
     ]
    }
   ],
   "source": [
    "print('Tokenizing...')\n",
    "inputs = tokenizer(graphs, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "print('Predicting...')\n",
    "y_hats = model.generate(inputs.input_ids)\n",
    "print('Decoding...')\n",
    "preds = tokenizer.batch_decode(y_hats, skip_special_tokens=True, clean_up_tokenization_spaces=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resulting in the following generated questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. who played darth vader in the film that matt lanter was a crew\n",
      "2. what is the attraction in madrid that has the batman : la fuga ride\n",
      "3. where does the owner of ecstasea live?\n"
     ]
    }
   ],
   "source": [
    "[print(f'{i+1}. {e}') for i,e in enumerate(preds)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
